{
  "question": "Mobileye CES 2024 product highlight and specification",
  "response": {
    "query": "Mobileye CES 2024 product highlight and specification",
    "follow_up_questions": null,
    "answer": null,
    "images": null,
    "results": [
      {
        "title": "Mobileye at CES 2024 | Mobileye Press Kit",
        "url": "https://www.mobileye.com/press-kit/mobileye-at-ces-2024/",
        "content": "Visit our press kit throughout the show for our latest news, videos, photos and more. Mobileye: Now. Next. Beyond. with President & CEO Prof. Amnon Shashua will be Presented on January 9, 2024, at 11:00 a.m. PT. Mobileye returns to Las Vegas for CES 2024, demonstrating progress and innovation on our path to delivering an evolutionary vision of ...",
        "score": 0.93085,
        "raw_content": "About\nSolutions\nTechnology\nCEO Corner\nNewsroom\nCareers\nInvestors\nContact Us\npress-kit\n|\nJanuary 02, 2024\nMobileye at CES 2024\nVisit our press kit throughout the show for our latest news, videos, photos and more.\nMobileye: Now. Next. Beyond. with President & CEO Prof. Amnon Shashua will be Presented on January 9, 2024, at 11:00 a.m. PT.\nMobileye returns to Las Vegas for CES 2024, demonstrating progress and innovation on our path to delivering an evolutionary vision of autonomy.\nVisit mobileye.com/ces-2024 for complete information on all of our CES activities.\nMobileye CES 2024 News\nMobileye Announces CES 2024 Press Conference with Prof. Amnon Shashua (Press Release, December 20, 2023)\nMobileye Reveals New Wins for Key Tech Platforms With Large Global Automaker (Press Release, January 8, 2024)\nMobileye Expands Collaboration With Mahindra and Mahindra to Explore Next-Generation Advanced Driving Technology (Press Release, January 9, 2024)\nPresentations\nThe full spectrum of Mobileye solutions, from driver assistance to autonomous driving. (Product Brief)\nMobileye: Now. Next. Beyond. CES 2024 Press Conference with Prof. Amnon Shashua (Presentation)\nMobileye's Driving Experience Platform: Architecture, Abstractions, and APIs, presented by Mobileye CTO Prof. Shai Shalev-Shwartz (Presentation)\nPhotos\nMobileye at CES 2024\nMobileye President & CEO Prof. Amnon Shashua presents at CES 2024\nThe Mobileye booth at CES 2024\nThe all-electric Volkswagen ID. Buzz equipped with Mobileye Drive\u2122\ufe0f self-driving technology on display at CES 2024\nThe Polestar 4, equipped with Mobileye SuperVision\u2122 technology, on display at CES 2024\nThe Polestar 4, equipped with Mobileye SuperVision\u2122 technology, on display at CES 2024\nMobileye President & CEO Prof. Amnon Shashua presents at CES 2024\nThe Mobileye booth at CES 2024\nThe all-electric Volkswagen ID. Buzz equipped with Mobileye Drive\u2122\ufe0f self-driving technology on display at CES 2024\nThe Polestar 4, equipped with Mobileye SuperVision\u2122 technology, on display at CES 2024\nMobileye President & CEO Prof. Amnon Shashua presents at CES 2024\nThe Mobileye booth at CES 2024\nThe all-electric Volkswagen ID. Buzz equipped with Mobileye Drive\u2122\ufe0f self-driving technology on display at CES 2024\nThe Polestar 4, equipped with Mobileye SuperVision\u2122 technology, on display at CES 2024\nMobileye's Advanced Platforms in the Driver's Seat\nA Polestar 4 equipped with Mobileye SuperVision\u2122\nThe Polestar 4 demonstrating Mobileye SuperVision\u2122 in Tel Aviv, Israel\nA Polestar 4 equipped with Mobileye SuperVision\u2122\nA Zeekr 001 equipped with Mobileye SuperVision\u2122\nA Zeekr 001 equipped with Mobileye SuperVision\u2122\nA Zeekr 001 equipped with Mobileye SuperVision\u2122\nAn all-electric Volkswagen ID. Buzz equipped with Mobileye Drive\u2122\ufe0f self-driving technology in Munich, Germany\nAn all-electric Volkswagen ID. Buzz equipped with Mobileye Drive\u2122\ufe0f self-driving technology in Munich, Germany\nAn all-electric Volkswagen ID. Buzz equipped with Mobileye Drive\u2122\ufe0f self-driving technology in Munich, Germany\nA Polestar 4 equipped with Mobileye SuperVision\u2122\nThe Polestar 4 demonstrating Mobileye SuperVision\u2122 in Tel Aviv, Israel\nA Polestar 4 equipped with Mobileye SuperVision\u2122\nA Zeekr 001 equipped with Mobileye SuperVision\u2122\nA Zeekr 001 equipped with Mobileye SuperVision\u2122\nA Zeekr 001 equipped with Mobileye SuperVision\u2122\nAn all-electric Volkswagen ID. Buzz equipped with Mobileye Drive\u2122\ufe0f self-driving technology in Munich, Germany\nAn all-electric Volkswagen ID. Buzz equipped with Mobileye Drive\u2122\ufe0f self-driving technology in Munich, Germany\nA Polestar 4 equipped with Mobileye SuperVision\u2122\nThe Polestar 4 demonstrating Mobileye SuperVision\u2122 in Tel Aviv, Israel\nA Polestar 4 equipped with Mobileye SuperVision\u2122\nA Zeekr 001 equipped with Mobileye SuperVision\u2122\nA Zeekr 001 equipped with Mobileye SuperVision\u2122\nA Zeekr 001 equipped with Mobileye SuperVision\u2122\nAn all-electric Volkswagen ID. Buzz equipped with Mobileye Drive\u2122\ufe0f self-driving technology in Munich, Germany\nAn all-electric Volkswagen ID. Buzz equipped with Mobileye Drive\u2122\ufe0f self-driving technology in Munich, Germany\nVideos\nMobileye 2024 Broll\nIntroducing Mobileye DXP\nMobileye: Now. Next. Beyond. CES 2024 Press Conference with Prof. Amnon Shashua (Replay)\nMobileye's Driving Experience Platform: Architecture, Abstractions, and APIs. (Replay)\nMobileye Brand\nMobileye logo horizontal\nMobileye logo stacked\nMobileye logo stacked\nMobileye logo horizontal\nMobileye logo stacked\nMobileye logo horizontal\nMobileye logo stacked\nShare article\nSign Up for Our Newsletter\nTo stay up-to-date on the latest at Mobileye\nIncorrect email address\nRelated News\nnews\nPolestar selects Mobileye to bring autonomous technology to Polestar 4\nAugust 25, 2023\nblog\nThe Road to the Future of Mobility is Being Mapped by REM\u2122\nJuly 25, 2023\nblog\nWhat is an Advanced Driver-Assistance System (ADAS)?\nJune 15, 2023\nPress Contacts\nContact our PR team\n\u00a9 2023 Mobileye\n\u00a9 2023 Mobileye"
      },
      {
        "title": "On the Fast Track to Autonomous Driving: Mobileye - CES",
        "url": "https://www.ces.tech/events-programs/ces-tech-talk-podcast/season-7/on-the-fast-track-to-autonomous-driving-mobileye.aspx",
        "content": "This is CES Tech Talk. I'm James Kotecki. CES 2024 is January 9th through 12th in Las Vegas, and it's already time to build the hype, so let's get smart about the world's most influential tech event. And let's be honest, you can't talk about CES without talking about self-driving cars.",
        "score": 0.92592,
        "raw_content": "Topics\nAbout\nTopics\nSessions & Events\nExhibits\nLogistics\nInformation for:\nArticles\nCES Tech Talk Podcast\nDiversity & Inclusion\nHuman Security for All\nOn the Fast Track to Autonomous Driving: Mobileye\nSubscribe\nCES 2024\nIt\u2019s coming your way, and to the highway. It\u2019s an autonomous vehicle (AV) platform that will change the driving experience as we know it. Autonomous vehicle tech company Mobileye is launching a platform that dramatically simplifies transitioning regular cars to operate autonomously. Hear from Mobileye SVP Nimrod Nehushtan as he discusses how far the tech has come in making autonomous driving reliable and feasible. Companies like Volkswagen and Porsche have aligned with Mobileye and the AV-driving future. Should you? Find out.\nIn this episode, you\u2019ll:\nGet a drivers-seat view of the Mobileye platform as its road tested\nFind out what\u2019s in the platform, and how it can be integrated into essentially any car\nBetter appreciate the technical sophistication involved in bringing safe autonomous driving to consumers\nNimrod Nehushtan\nSenior Vice President Business Development & Strategy and Co-Manager REM, Mobileye\nNimrod Nehushtan is responsible for Mobileye\u2019s strategic planning and initiatives, business objectives and development, as well as advanced development projects. In addition, as REM co-general manager, he is responsible for defining the productization of Road Experience Management\u2122 (REM) for OEM customers, leading REM product, delivery, and business engagements to optimize Mobileye\u2019s mapping solutions and services and maximize their reach in the ADAS and AV markets. Prior to joining Mobileye, Nehushtan worked as an engineer at Israel Aerospace Industries.\nNehushtan received a bachelor's degree cum laude in mechanical engineering from Tel Aviv University.\nGrace Venes-Escaffi (00:00):\nAre you ready to unlock the power of innovation? Become a member of the Consumer Technology Association and engage with a community of innovators. As a CTA member, you'll access cutting-edge research, connect with industry leaders, and shape industry standards, enjoy exclusive discounts, CES perks, and invaluable networking opportunities. Visit cta.tech and be a part of advancing the technology industry.\nJames Kotecki (00:31):\nThis is CES Tech Talk. I'm James Kotecki. CES 2024 is January 9th through 12th in Las Vegas, and it's already time to build the hype, so let's get smart about the world's most influential tech event. And let's be honest, you can't talk about CES without talking about self-driving cars. In the past, on this podcast, we've covered autonomous race cars, but when will the autonomous revolution be speeding down your street? We're going to look at the state of the industry and the future of everyday driving with Nimrod Nehushtan. He's senior vice president strategy and business development at Mobileye, an autonomous vehicle technology company. Nimrod, welcome to CES Tech Talk.\nNimrod Nehushtan (01:15):\nWelcome, James. Thank you very much for having m, I'm looking forward for this talk today.\nJames Kotecki (01:21):\nAnd let's start by contextualizing for folks the piece of the autonomous vehicle challenge, or the pieces of the challenge, that Mobileye is actually working on. What's the easy way for the general CES audience to understand how Mobileye fits into the picture?\nNimrod Nehushtan (01:37):\nYeah, that's a very good question to start with. So essentially, we are developing a platform that can pretty much be integrated into any car, that will make this car fully autonomous. And this starts with a compute platform, so silicon and the different compute resources needed for the different technologies, as well as the perception stack, computer vision technologies, sensors, sensor fusion radars and LiDARs, mapping, through our crowdsourced mapping REM, our crowdsourced mapping activity, driving policy. And of course, also, we're developing active sensors, which will be a part of our future roadmap for next generation of autonomous vehicles. So you can pretty much say that we start from silicon design, all the way up to computer vision algorithms, sensor development, computer vision advancements, and mapping and driving policy. So pretty much the entire stack.\nJames Kotecki (02:36):\nSo when you're talking about any car becoming autonomous, just to be clear, are we talking about retrofitting older cars with all of this new equipment, or just newer cars?\nNimrod Nehushtan (02:46):\nThe idea is that, essentially, when you think about autonomous driving, you need to provide three major components in the system. There is the sensing layer, which is responsible for understanding the environment and the dynamic objects, road users, vehicles, pedestrians, cyclists, everything that is dynamic around the car. There is the mapping, which is traditionally responsible for understanding the road structure, the driving rules, lanes structure, traffic lights, everything that is static around the road itself, where the car is in. And then, based on top of these two elements, you have the driving policy, which is responsible for decision making and for the dos and don'ts of the autonomous vehicle.\n(03:30):\nAnd what we are, in our company, developing and providing, is basically a platform that includes all these three components, as well as the silicon design and the compute that is necessary to run those in a car. And we are working with car companies to integrate this platform into new cars that will be launched in the future, so that we can offer hands-off, eyes-off, driving functions to consumers, through our partnerships with car companies.\nJames Kotecki (03:59):\nI understand one of those partnerships is with Volkswagen, so can you maybe get a little bit more into the details of what this partnership means in practice there?\nNimrod Nehushtan (04:06):\nYeah, so in reality, I think we should go back a little bit further in time to look at the partnership with Volkswagen. We have been working, and Volkswagen has been one of our major partners for a few years now, and it started with what we call basic driving assist products, which is what everyone is familiar with today, automatic emergency braking for collision warning, lane departure warning, all these standard driving assist functions. And we have integrated our products, our EyeQ product, that provides all of these functions in many, many vehicles from Volkswagen Group over the years. And I think an important milestone in the relationship of the companies was, I think, six years ago, when we have jointly announced a partnership around crowdsourced mapping and REM, in which both companies collaborated in launching this innovative technology in the market, and it has been a very big success for both companies.\n(05:03):\nAnd recently, we have announced a partnership with Porsche, to integrate our SuperVision product into Porsche vehicles in the next few years. SuperVision is our platform that provides hands-free driving, which means, basically, it's fully capable of all the driving tasks completely autonomously, but it's not a eyes-off system, which means that you are the driver, you will be allowed to take your hands off the wheel, but you cannot take your eyes off the road, and you still need to be engaged and alert of what's coming. But in terms of the system capabilities and the overall robustness of the system, it is very, very advanced in what it can offer to driving, and it will completely change the driving experience for us as drivers.\nJames Kotecki (05:47):\nYou've used these terms hands-off, eyes-off, a couple of times, so I want to get into this, because I think this is something that is specific to Mobileye. In other words, I think you're using those terms very deliberately, because my understanding is that, at the previous CES, CES 2023, Mobileye unveiled these terms as a way of making it much more understandable to people, in non-technical terms, what this technology does. It's interesting, even in this conversation so far, you're mentioning a bunch of advanced technologies, but you're also mentioning these simple terms. So tell me more about why Mobileye decided to do that, what those terms are, and how it's going for you in using those in the real world.\nNimrod Nehushtan (06:23):\nWell, I think maybe a change in mindset that we've had is that we are thinking about what the system can and cannot do in the eyes of the consumer, and under the assumption that the average consumer is not extremely up to date with the latest regulation and standards that the industry is using, which is more phrased in an engineering language. And what we have found is that, of course, being involved in this activity for a few years now, is that the common language in the industry that is more engineering related got many different types of interpretations. And especially when it comes to the difference between level two plus, level three, level four, you can find varying opinions on what are the nuances that differentiate between this level and the other.\n(07:10):\nAnd we wanted to keep it simple so that the broad audience can understand what this system is capable of. And this comes from a very, as you said, deliberate notion that we want it to be clear for consumers on what they can and cannot do when they will be using the system. And it means, for us, basically, we divide it to eyes-on, hands-on. This is a normal car today where the system is designed to help you, as a driver, to prevent dangerous events, to provide alerts and warnings and so on, but you still drive all the time. You're engaged and you're holding the wheel.\n(07:47):\nThe next step from there is what we call eyes-on, hands-off, which is our SuperVision system. In that case, as a driver, once you activate the system, it will drive completely autonomously, but as a driver, you need to still be engaged and be looking at the road, so that you're basically supervising and monitoring the system, and not physically driving all the time. This is very similar to how pilots are flying planes today. You have the autopilot, it's driving, or it's flying the plane, but as a pilot, you still need to monitor it and see that everything is functioning properly. And this is the eyes-on, hands-off SuperVision system.\n(08:26):\nFrom that point, we moved to eyes-off, hands-off. And for us, once we say eyes-off, you need to really assume that once you allow the driver to take his or her eyes off the road, the system needs to be extremely robust and to be able to handle cases where maybe the driver is not alert at one second notice, or even ten second notice, in some cases. So it's not enough just to allow the driver to take their eyes off the road, you need to also design the system to handle cases where maybe the driver is not there to take control, or is not responsive enough, which is what we call minimum risk maneuvers, and reaching a safe stop, and so on. So for us, again, it's very clear, you as a driver, you know, okay, once the system is active, I can take my eyes off the road and be minding my own business, reading my phone and doing mails, whatever it may be.\n(09:19):\nAnd then, the final step is no driver, this is like a robo taxi where there's no driver. And this is the complete spectrum of products, and how you can move from eyes-on, hands-on, which is the common case today in most cars, and all the way up to no driver.\nJames Kotecki (09:37):\nI recall a debate in the industry about whether and how much assistance to give folks in that middle section between I'm fully driving the car myself and there's no driver. And I remember the debate going something along the lines of, on the one hand, you had people who said, \"We need to gradually ramp this up and give people more and more assistance, how the technology is going to progress.\" And then, on the other hand, you had folks who said, \"We have to go from, I'm driving the car completely myself, all the way to no driver, it almost has to be a switch.\" Because people in that interim period where, let's say they're hands-off, eyes-on, for example, to use your terminology, they are going to drift, they're not going to be alert, they're going to forget that they have to take control, they're going to be lulled, in other words, into a false sense of security almost, and in some ways, that is more dangerous.\n(10:24):\nSo what's the state of that debate today? I think it's somewhat clear what your answer for that is, but why are we going in this direction?\nNimrod Nehushtan (10:32):\nWell, I think this debate is very much real today, and it's present in many debates in the industry. And there has been numerous events, there has been public events, in which specific behaviors of different systems led to this discussion even becoming more and more relevant, as to whether it's clear enough or not to drivers on what they can and cannot do when the system is active.\nJames Kotecki (10:32):\nYeah.\nNimrod Nehushtan (10:59):\nAnd I think this is an extremely important topic, and the more these systems become autonomous, I think first of all, it needs to be clear to the driver as to the dos and don'ts once the system is active. And in addition to that, a driver monitoring system is a very, very important piece of the system, so by having a technology that can inspect and monitor the driver, you can pretty much anticipate events in which drivers will maybe abuse the system or will maybe simply fall out of concentration.\n(11:37):\nSo we have technologies like this today, and these are very, very important pieces of the entire system, because you can see if somebody's getting drowsy, if somebody's getting unconcentrated, if somebody's sneak peeking into his phone, even if he's not supposed to do that, and you can disengage and you can honk the horns or whatever it may be, and at least mitigate this risk. So it's not enough just to say so, at least in our view, it's not enough just to say so in the terms and conditions when you buy the car, you need to also have an active system that can continuously inspect the driver and make sure that the proper use of the system is being made.\nJames Kotecki (12:15):\nRight. It seems like there's the, \"Well, I told him not to do it, and he did it, but he should have known better.\" But of course, the consequences of that can be significant.\n(12:24):\nI also wonder if the necessity of the approach that you are outlining, which sounds like you're gradually ramping up the amount of assistance that you're giving a driver, up until the point that you can just take the driver out of the seat completely, I wonder if that's necessitated by the fact, and you tell me if I'm wrong, that fully autonomous, driverless, no driver, that stage is just very hard to achieve on a practical level. For years, we've seen videos of cars with no drivers, driving around in certain, maybe with a bit of marketing polish on it, but it seems to me that this dream of not having a driver in the car, it seems to just be a lot harder to have achieved than maybe people would've thought five or 10 years ago. Is that a fair assessment of where things are in the industry today?\nNimrod Nehushtan (13:09):\nWell, I think it's pretty safe to say that there is definitely some level of realization that maybe some of the challenges that were previously believed to be simpler, wanted to be more complicated. But I don't think necessarily the question of when will we have driverless cars everywhere is a very tricky one, and you can have an argument one way or the other. I think a more interesting debate is, what will be the meaningful milestones in the path towards that end state?\n(13:41):\nAnd I think what might be understated in the discussion is that there are a few major milestones in the path towards driverless cars that will completely change how we are consuming transportation and how we are experiencing the car. And these are much more imminent, let's say, and much more realistic today, although we can argue that it's not necessarily decades away. But regardless of the question of when driverless cars will be as prevalent as they were previously thought to be, I think that in the next three to five years, we see major leaps in what commercially available system will offer to drivers, and talking about the hands-off applications that will provide new levels of comfort and much less anxiety, and we also believe better safety overall to drivers.\n(14:39):\nAnd then, conditional autonomy, which is practically eyes-off, but in limited conditions, can be extremely useful. So imagine a scenario where you're entering your car and you're selecting a destination in the navigation system, and then once you start driving, you start driving hands-off. So the system takes you from your house and drives the small residential streets fully autonomously and you simply inspect it. And then, once you enter the highway, it enters a new stage which is eyes-off. You have a warning that says, \"Okay, now you can take your eyes off as well.\" And then, all the way, once you're on the highway, including all the different use cases and scenarios, like overtaking a truck and obstacles and block planes and so on, it's fully autonomous in a way that you can be eyes-off. And then, once you approach the end of the highway, a few moments before that, there is a procedure in which the control is being transferred to the driver again, and then you continue with your journey.\n(15:44):\nThat alone, that segment of just autonomous driving in a highway, eyes-off, this is extremely useful and it's a complete game changer. Thinking about the amount of time we're wasting today, driving on highways in traffic jams and so on, it's only getting worse, thinking about the amount of accidents that we can prevent in high speeds, which is almost always fatal. So the overall impact to society, even if we only provide this level of autonomy, is night and day compared to today. And this is within reach, we are actively working, we're busy solving the problems towards that, towards this product, and we see this launching within the next three to five years. So we don't think of this as a far future type of product. And just imagining having this kind of a product available in the market, I think, is a real game changer.\nJames Kotecki (16:38):\nAnd do you think we'll be able to see those milestones show up in the derivative statistics there, in terms of productivity statistics, you mentioned accident statistics? Is there a data point milestone on something like that, that you're looking for to say, \"Okay, this is a sign that these gains that you're talking about have actually arrived and been locked in\"?\nNimrod Nehushtan (17:01):\nYeah, I think there are, and I think the most obvious ones will be the amount of time people are wasting and driving on average.\nJames Kotecki (17:09):\nYeah.\nNimrod Nehushtan (17:10):\nThis can be reduced. If you think about the common journey or the common commute, there is a significant portion of that that is on highways. And also, a lot of the traffic jams are concentrated in highways, and also just long parts of the journey are in highways. This is how the road network is built. And I think that in terms of the amount of time people are wasting, not just sitting in the car, but wasting time while you're in the car, will be reduced.\n(17:42):\nAnd also, safety. We're talking about, at least in our view, the threshold to launch these products is to be orders of magnitude better than humans, statistically. In terms of the probability of a fatal accident or any type of accident, these systems are not going to be as good as humans to be accepted by society. They need to be orders of magnitude better. So the overall impact to society, the more these systems will become prevalent, the more you will see a drop in the amount of fatal accidents. And I think, as we all know, accidents and car crashes today are one of the leading factors for early fatalities.\nJames Kotecki (18:24):\nYeah.\nNimrod Nehushtan (18:24):\nSo there is no way to reduce that to zero without taking the driver out of the equation. And the more the systems will become autonomous, and the more drivers will be out of the equation for a longer and longer portion of the drive, the more you will see this number of fatal accidents drop dramatically. I think a good example could be aviation. If you compare today the statistics of being involved in a fatal plane crash compared to 50 years ago, it's orders to magnitude different. And one of the biggest revolution that happened in aviation is the level of automation that entered. 50 years ago,, you could be boarding a plane and the pilot will say, \"Listen, guys, I'm lost. I don't know where we are. I hope we have enough gas.\" So the more automation and the more sophistication and compute entered planes, the room for error of a pilot was mitigated.\nJames Kotecki (19:23):\nAnother data point we could look at, as this technology increases in terms of prevalence and adoption, is people's attitudes about using it. And so, do you look at data for that, for people's willingness to take their hands and eyes off the road? Is that changing, and what do you see as far as the trends there?\nNimrod Nehushtan (19:45):\nI think that's a very good question. I've been personally using our cars on a day-to-day basis, and once you get used to it, and it takes a relatively short amount of time to get used to it, it's pretty hard to imagine going back to driving normally again, driving manually again. And I think there is a level of trust that needs to be gained between the user and the system, that after a few hours of driving, you're experiencing the system in different scenarios and you understand how to engage with it, you develop a certain level of trust, and then you get used to the comfort.\n(20:20):\nNow, today, again, I'll give an example, think about cruise control. This is so standard today that it's available in every car, but look back 15 years ago, not everyone treated it lightly when it came to counting on the car to keep the gas and brake pedals as they are. Today, you don't even think about this. And when we think about enabling these systems, it will be the same process. After a few hours of driving it and seeing the performance, seeing the limitations and strengths of this, you start to understand that, in many ways, this system is actually better than you, and you would actually prefer the system to be driving, in certain scenarios, in pretty much most scenarios, once you get used to it and you see it working in the field. And it's not a theory, this is in practice what happens with people that are starting to dip their toes into this.\nJames Kotecki (21:19):\nYeah.\nNimrod Nehushtan (21:20):\nBecause we, as humans, we are very, very bad in driving in mundane scenarios, like driving on a long highway for a few hours, maybe we get exhausted, we get unconcentrated. And you see that the system excels in these situations, and it's simply easy for you to let go, because you see how much more reliable the system is compared to you, driving back from work, being tired and have a two-hour commute.\nJames Kotecki (21:51):\nYeah.\nNimrod Nehushtan (21:51):\nIt's pretty easy, because the value that you get from the system is so high, and after a few hours of gaining trust, it's a no-brainer.\nJames Kotecki (21:59):\nIt's one thing to get used to it for your own car, and I think maybe societal acceptance is maybe a separate question, I'm not sure if you agree with that, but I just wonder, because you mentioned the idea of having to be, or wanting to be, orders of magnitude better than a human driver just to launch your technology. Now, if you just looked at it from a purely mathematical point of view, if you were just one percentage point better than a human driver, that would be better, from just a raw mathematical perspective. But of course, there's this idea, this perception, and you mentioned some of these cases made the news, if the robot car crashes and someone is killed, then that's a headline news and people maybe have this mistrust over that, versus there's obviously day-to-day crashes that kill many people that humans are involved in and that doesn't make the news. So how does human perception play into this need to be so much better than human drivers?\nNimrod Nehushtan (22:52):\nYeah, I think this is a very important question. One of our fundamentals in our autonomous vehicle stack, let's say, in our offering, is that we believe that when it comes to the tolerance for errors is different for different circumstances. I'll explain. We think that it's much more likely that nobody will tolerate errors because of wrong decision-making. So for example, as a human, if somebody decides to run over somebody else, by actively deciding to do so on purpose, it's a different felony than, if by mistake, something unexpected happened and you could not do anything better, but it still led to an accident. So when it comes to errors because of wrong decision-making, this will not be tolerated.\n(23:46):\nAnd I think maybe, unlike most of other companies that are trying to, or working on, autonomous vehicles, we believe that decision-making, the driving policy which is responsible for the dos and don'ts of the system, this needs to be a very transparent and easy to understand element. It cannot be a black box and a heavy AI driven machine, because it's very hard to understand why AI system decided to do what it decided to do at some point in time. It's very hard to debug it, and to read into the code, and to understand the policy of it. If you have a transparent model, which our driving policy is based on our proprietary responsibility sensitive safety concept, RSS, we have published papers, white papers on this, you can read it online, you can understand exactly the rules in which we are coding our system to behave. And also, we can easily understand why the system decided to do what it decided to do at any point in time. And therefore, we can prove that there could not be any wrong decision making that will lead to dangerous events because of the system.\n(24:57):\nAnd this is a critical element to gain the trust of the general society, and we're actively working on exposing this concept and creating a consensus that these elements needs to be transparent, and maybe there is a call for a common standard in the industry around that. So at least you can say, \"This is how the system is going to do what it decided to do. This is the rules of the game.\" It's not an unexpected statistic approach that says, \"We hope we learned enough data. We hope we used enough data to train our AI system, and we hope it's statistically good enough.\" We think that when it comes to decision-making, there is no room for hope, it needs to be definitive, and this is a core element in our system.\n(25:41):\nAnd when it comes to other elements, I think the more the systems will be prevalent, the more we can see that overall, the benefit to society is so dramatic and the type of errors are going to be explainable and understandable. Again, taking back an example from aviation, there is a certain probability that the wings will fall off the plane. If it happens, and you read about it in the news, it's a tragedy, of course, but all of us know, it can happen in some probability. But if you read in the news that the pilot decided to crash the plane on purpose, it's a completely different story. So this is our approach, and this is the fundamentals and guidelines in our design of the system.\nJames Kotecki (26:27):\nAnd of course, the probability of that example, the wings falling off the plane, is obviously vanishingly small, so we accept it as a society, and it would definitely be surprising if that were to happen. You mentioned industry standards. The next level up from that is regulatory and legal standards. Are there certain things, I believe you're coming to us from Israel right now, is that right?\nNimrod Nehushtan (26:49):\nYes.\nJames Kotecki (26:50):\nSo I'm here in the United States, but all around the world, lawmakers and regulators are going to have to be grappling with this. Where do you see folks who are leading on this issue? Do you see the regulatory and legal environment shaping up to where this stuff can be adopted on a mass scale and there is that standardization?\nNimrod Nehushtan (27:13):\nI think there is definitely a discussion in the industry about this, about the need for regulation and standards, and we're closely watching how this progresses, and we also think it's going to be a key in future launches of these products in large scale. And again, we're already seeing this, the [inaudible 00:27:33] collaboration and ongoing engagements in the market, in order to promote this, to create some certainty, and to create some standards that can then be used by technology companies, by whoever it may be in the industry, car companies, service operators, to at least understand what will be the criteria to launch these products in a way that has some level of assurance behind it. And this will be very important to balance the risk and the dilemmas that maybe some companies are facing today because of the certainty around it.\nJames Kotecki (28:08):\nIs there a regulatory track or a legislative philosophy that you are especially wary of? In other words, is there some politician out there saying, \"We have to just ban all self-driving cars\"? I haven't heard of that. It seems like we're debating philosophically within a range of possibilities, all of which would eventually lead us toward a more autonomous future, but you probably have a better sense of this than me.\nNimrod Nehushtan (28:32):\nNow, I'm not aware of any deliberate ongoing engagement in high profile around completely banning autonomous vehicles altogether. I think looking at the long-term, we have to have these technologies and we have to have these products. This can be a dramatic positive impact to society, and the industry is investing in changing the way we are consuming transportation, and regulation needs to also join the party and help in creating the standards and certainty to make it a commercially available proposition. So I think that we're not aware of any discussion that tries to pull the ship in the opposite direction, at this point.\nJames Kotecki (29:22):\nSo as you mentioned, your company is based in Israel. We're looking forward to having you join us in Las Vegas for CES 2024. You're going to have to come halfway around the world for that. Why do you come to CES and what are you planning for CES 2024?\nNimrod Nehushtan (29:34):\nSo first of all, CES is an amazing platform for us to not only see our colleagues in the industry, but also get up to speed with the latest technologies, meet startups, meet entrepreneurs, and see the different innovations coming in many, many different type of markets. We are always excited about the opportunity to meet our customers and counterparts, and it's very, very effective for us, so it's a no-brainer for us to come halfway around the world for CES, and we plan to do that again in 2024. This year, we will, again, show the advancements in our technology, latest status of the business, share our vision and progression in the autonomous vehicle journey. So a lot to look forward to, and we are very excited about 2024 this year.\nJames Kotecki (30:23):\nThank you so much for joining us. Nimrod Nehushtan, Mobileye, really appreciate you being here on CES Tech Talk.\nNimrod Nehushtan (30:31):\nThank you very much, James, for having me.\nJames Kotecki (30:33):\nAnd thank you so much for listening and/or watching. That's our show for now, but there's always more tech to talk about, so please subscribe to this podcast so you don't miss a moment. You can get even more CES and prepare for Vegas at ces.tech. That's ces.tech. Our show today is produced by Nicole Vidovich and Mason Manuel, recorded by Andrew Lynn and edited by Third Spoon. I'm James Kotecki, talking tech on CES Tech Talk.\nCES is owned and produced by the Consumer Technology Association, which provides the ultimate platform for technology leaders to connect, collaborate, and propel consumer technology forward.\nBecome a CTA Member\nAbout CES\nCES Events\nInnovation Awards\nCES Tech Talk Podcast\nPromote Your Brand\nTopics\nArticles\nCES Success Stories\nSchedule\nOur Partners\nInformation for:\nExhibitors\nMedia\nInternational\nFollow CES\nCode of Conduct\nTerms of Use\nPrivacy\nSitemap\nCopyright \u00a9 2003 - 2024. All rights reserved."
      },
      {
        "title": "Mobileye at CES 2024",
        "url": "https://www.mobileye.com/ces-2024/?hsa_acc=5573234516&hsa_cam=19242289908&hsa_grp=147205323529&hsa_ad=641199324123&hsa_src=g&hsa_tgt=kwd-3921568492&hsa_kw=mobileye&hsa_mt=e&hsa_net=adwords&hsa_ver=3",
        "content": "Mobileye SuperVision\u2122 is our most advanced driver-assistance system, derived directly from our autonomous vehicle development program, enabling \"hands-off, yet eyes-on\" on a variety of road types. It is a full surround-view camera-based system that serves as a bridge to a higher level of autonomy. Cameras. Main front-facing 8MP camera.",
        "score": 0.91399,
        "raw_content": "About\nSolutions\nTechnology\nCEO Corner\nNewsroom\nCareers\nInvestors\nContact Us\nJANUARY 9-12, 2024#4600 WEST HALL & VIRTUAL\nMobileye at CES 2024\nMobileyeat CES 2024\nDriving the autonomous vehicle evolution\u2122\nOur leadership at CES\nOur leadershipat CES\nIN THE NEWS\nVisit our CES Press Kit for the latest news\nKeep up with Mobileye\u2019s CES 2024 events, announcements and visuals.\nCES 2024 News\nMobileye DXP\nThe Driving Experience Platform\nMobileye DXP enables automakers to build on its framework and code a unique end-product for their customers. This ability allows automakers to reduce time-to-market and deliver a driving experience that reflects their brand.\nIncremental and Modular\nIncrementaland Modular\nThe shift from ADAS to consumer AV is an incremental and modular process. Mobileye builds upon the technology foundations of RSS\u2122, REM\u2122 and 360\u00b0 degree surround sensing to reach full autonomy. This allows automakers tailor their advanced driving systems to their brand identity while enhancing their autonomous capabilities gradually.\nMobileye SuperVision\u2122\nHands-off | Eyes-on\nMobileye Chauffeur\u2122\nHands-off | Eyes-off\nVARIOUS ROAD TYPES*\nHIGHWAY\nARTERIAL & RURAL\u200b\nURBAN\n360\u00b0 Vision\nREM\u2122\nRSS\u2122\n2 x EyeQ\u2122 6 High\n*Operates within specified operational design domain, and subject to local law and regulation\n3 x EyeQ\u2122 6 High\nFront lidar\u200b and surround radars\n3 x EyeQ\u2122 6 High\nImaging radars for greater redundancy\u200b and front lidar\n4 x EyeQ\u2122 6 High\nImaging radars for greater redundancy\u200b and front lidar\nMobileye SuperVision\u2122\nHands-off | Eyes-on\nMobileye Chauffeur\u2122\nHands-off | Eyes-off\nVARIOUS ROAD TYPES*\n2 x EyeQ\u2122 6 High\n*Operates within specified operational design domain, and subject to local law and regulation\nHIGHWAY\n3 x EyeQ\u2122 6 High\nFront lidar\u200b and surround radars\nARTERIAL & RURAL\u200b\n3 x EyeQ\u2122 6 High\nImaging radars for greater redundancy\u200b and front lidar\nURBAN\n4 x EyeQ\u2122 6 High\nImaging radars for greater redundancy\u200b and front lidar\n360\u00b0 Vision\nREM\u2122\nRSS\u2122\nGet an up-close view of the tech that drives us forward\nMobileyeSuperVision\u2122\nMobileyeChauffeur\u2122\nMobileyeDrive\u2122\nMobileye SuperVision\u2122 is our most advanced driver-assistance system, derived directly from our autonomous vehicle development program, enabling \u201chands-off, yet eyes-on\u201d on a variety of road types. It is a full surround-view camera-based system that serves as a bridge to a higher level of autonomy.\nCameras\nMain front-facing 8MP camera\nNarrow front-facing 8MP camera\n2 x side-front 8MP cameras\n2 x side-rear 8MP cameras\nRear 8MP camera\n4 x 2MP short-range surround cameras\n*Front radar is optional\nDUAL EYEQ\u21225/6 HIGH\nDual EyeQ\u21225/6 High chips implemented in a full end-to-end Mobileye-designed ECU, including hardware architecture, functional stack, decision layers, and end-user functions\nROAD EXPERIENCE MANAGEMENT\u2122 (REM\u2122)\nProvides crowdsourced maps with detailed information on road features and local driving behavior, complementing the data captured by sensors and strengthening its accuracy\nRESPONSIBILITY-SENSITIVE SAFETY\u2122 (RSS\u2122)\nAn open, comprehensive, and verifiable mathematical safety model for driving that\u2019s part of the SAE standard for real-world AV deployment\nGet an up-close view of the tech that drives us forward\nMobileyeSuperVision\u2122\nMobileyeChauffeur\u2122\nMobileyeDrive\u2122\nMobileye SuperVision\u2122 is our most advanced driver-assistance system, derived directly from our autonomous vehicle development program, enabling \u201chands-off, yet eyes-on\u201d on a variety of road types. It is a full surround-view camera-based system that serves as a bridge to a higher level of autonomy.\nCameras\nMain front-facing 8MP camera\nNarrow front-facing 8MP camera\n2 x side-front 8MP cameras\n2 x side-rear 8MP cameras\nRear 8MP camera\n4 x 2MP short-range surround cameras\n*Front radar is optional\nDUAL EYEQ\u21225/6 HIGH\nDual EyeQ\u21225/6 High chips implemented in a full end-to-end Mobileye-designed ECU, including hardware architecture, functional stack, decision layers, and end-user functions\nROAD EXPERIENCE MANAGEMENT\u2122 (REM\u2122)\nProvides crowdsourced maps with detailed information on road features and local driving behavior, complementing the data captured by sensors and strengthening its accuracy\nRESPONSIBILITY-SENSITIVE SAFETY\u2122 (RSS\u2122)\nAn open, comprehensive, and verifiable mathematical safety model for driving that\u2019s part of the SAE standard for real-world AV deployment\nExplore Mobileye\u2019s products and technology\nADAS\nBaseDriver-Assist\nHands-on | Eyes-on\nCloud-EnhancedDriver-Assist\u2122\nHands-on | Eyes-on\nMobileyeSuperVision\u2122\nHands-off | Eyes-on\nMobileyeChauffeur\u2122\nHands-off | Eyes-off\nMobileyeDrive\u2122\nNo driver\nAV\nAbout\nSensors\nUnique Mobileye Tech\nFeatures\nVideo\nBase Driver-Assist\nAbout\nMobileye\u2019s Base Driver-Assist, powered by our purpose-built EyeQ system-on-chip, brings our Advanced Driver Assistance Systems (ADAS) to millions of vehicles on the road today. This solution provides cost-effective features for regulatory compliance and improved road safety.\nCUSTOMERS\nMore than 50 of the world\u2019s leading automakers, with over 800 models on the market around the world.\nCloud-Enhanced Driver-Assist\u2122\nAbout\nMobileye Cloud-Enhanced Driver-Assist\u2122 leverages crowdsourced data from millions of Mobileye-equipped vehicles around the globe every day, providing centimeter-level localization through continuously updated information about the driving scene. It provides a safer, smoother, and more natural driving experience \u2013 marking a leap in ADAS performance but with no need for additional hardware.\nCUSTOMERS\nVWSkodaSEATFord\nMobileye SuperVision\u2122\nAbout\nMobileye SuperVision\u2122 is our most advanced driver-assist system on the market and the \u2018bridge\u2019 to consumer AVs. It is designed to handle standard driving functions across various road types, offering the \u201chands-off\u201d navigation capabilities of an autonomous vehicle, while still requiring the driver to pay full attention and keep eyes on the road.\nCUSTOMERS\nPorschePolestarFAWZeekrsmart\nMobileye Chauffeur\u2122\nAbout\nMobileye Chauffeur\u2122 is our safe and geographically scalable hands-off/eyes-off solution for consumer vehicles, combining computer vision technology with surround imaging radars and front lidar.\nCUSTOMERS\nPolestarFAW\nMobileye Drive\u2122\nAbout\nMobileye Drive\u2122 is our end-to-end self-driving system that enables automakers and transportation operators to offer a no-driver solution for robotaxis, ride-pooling, public transport, and goods delivery.\nCUSTOMERS\nAutomakersVWMANSchaefflerBenteler/ HOLONBeepTransportation operatorsHolo/ RuterSIXTDeutsche- Bahn\nBase Driver-Assist\nSensors\n1 front camera:\n120\u00b0 X 57 (+42,-15)FoV/up to 8MP\nCloud-Enhanced Driver-Assist\u2122\nSensors\n1 front camera:\n120\u00b0 X 57 (+42,-15)FoV/up to 8MP\nMobileye SuperVision\u2122\nSensors\n11 cameras, including:\n7 long-range cameras 8MP\n4 parking cameras 3MP*Additional radars are optional\nMobileye Chauffeur\u2122\nSensors\n11 cameras, including:\n7 long-range cameras up to 8MP\n4 parking cameras 3MPand\n5 surround imaging radars\n1 front lidar\nMobileye Drive\u2122\nSensors\n13 cameras, including:\n4 side cameras 8MP4 parking cameras 2MP3 front-facing cameras up to 8MP1 traffic light camera 8MP1 rear camera 8MPand5 surround imaging radars3 surround lidars\nBase Driver-Assist\nUnique Mobileye Tech\nMoblieye SoC:\nEyeQ\u2122 4M | EyeQ\u2122 5M | EyeQ\u2122 6L\nCamera-Based Technology\nMobileye\u2019s proprietary computer vision technologies\nCloud-Enhanced Driver-Assist\u2122\nUnique Mobileye Tech\nMoblieye SoC:\nEyeQ\u2122 4M | EyeQ\u2122 5M | EyeQ\u2122 6L\nCamera-Based Technology\nMobileye\u2019s proprietary computer vision technologies\nRoad Experience Management\u2122 (REM)\nMobileye\u2019s crowdsourced AV mapping technology\nMobileye SuperVision\u2122\nUnique Mobileye Tech\nMoblieye SoC:\nMobileye\u2019s ECU based on 2 x EyeQ\u2122 5/6H\nCamera-Based Technology\nMobileye\u2019s proprietary computer vision technologies\nRoad Experience Management\u2122 (REM)\nMobileye\u2019s crowdsourced AV mapping technology\nResponsibility-Sensitive Safety\u2122 (RSS)\nAn open source, comprehensive, and verifiable mathematical approach to balance safety and efficient driving\nMobileye Chauffeur\u2122\nUnique Mobileye Tech\nMoblieye SoC:\nMobileye's ECU Based on 3 or 4 x EyeQ\u2122 6H\nCamera-Based Technology\nMobileye\u2019s proprietary computer vision technologies\nRoad Experience Management\u2122 (REM)\nMobileye\u2019s crowdsourced AV mapping technology\nResponsibility-Sensitive Safety\u2122 (RSS)\nAn open source, comprehensive, and verifiable mathematical approach to balance safety and efficient driving\nTrue Redundancy\u2122\nMobileye\u2019s sensor fusion architecture based on two distinct independent systems: a camera system, and a radar-lidar system\nMobileye Drive\u2122\nUnique Mobileye Tech\nMoblieye SoC:\nMobileye's ECU Based on 4 x EyeQ\u2122 6H\nCamera-Based Technology\nMobileye\u2019s proprietary computer vision technologies\nRoad Experience Management\u2122 (REM)\nMobileye\u2019s crowdsourced AV mapping technology\nResponsibility-Sensitive Safety\u2122 (RSS)\nAn open source, comprehensive, and verifiable mathematical approach to balance safety and efficient driving\nTrue Redundancy\u2122\nMobileye\u2019s sensor fusion architecture based on two distinct independent systems: a camera system, and a radar-lidar system\nBase Driver-Assist\nFeatures\nNCAP functions \u200b(Automatic Emergency Braking, Forward Collision Warning, Traffic Sign Recognition, \u200bLane Keep Assist)\u200b\u200b\nVision-only GSR ISA compliance\nAutomatic Cruise Control (ACC)\u200b\u200b\nLane centering on marked roads\u200b\u200b\nLane departure warning\u200b\u200b\nCloud-Enhanced Driver-Assist\u2122\nFeatures\nMobileye Base Driver Assist features plus:\u200b\u200b\nLane centering on unmarked roads\u200b\nTraffic light/sign relevancy \u200b& warning\nACC at high speeds, under changing road conditions\u200b\u200b\nOccluded traffic sign support\nAdvanced construction area and road/hazard warnings\nAutomatic speed adjustment according to traffic signs\nMobileye SuperVision\u2122\nFeatures\nMobileye Cloud-Enhanced ADAS\u200b\u2122 features plus:\u200b\u200b\nHands-off navigation for various ODDs - included in-scope, highways, arterial, rural, and urban roads, at up to 130 kph\u200b\nAutomatic lane change\u200b\u200b\nAcceleration/deceleration in response to other drivers cutting in\u200b\u200b\nFront & rear collision avoidance\u200b\u200b\nAutomatic minimum risk maneuver\u200b\u200b\nDMS, parking & other applications on a single EyeQ\u2122 chip\u200b\u200b\nOTA upgrades\nDriving Experience Platform\u2122 (DXP\u2122)\nMobileye Chauffeur\u2122\nFeatures\nMobileye SuperVision\u2122 functions plus:\u200b\u200b\nEyes-off NOP platform for point-to-point navigation in various ODDs, including highways (up to 130 kph), arterials, rural, and urban roads\nAutomatic safe stop on road shoulder\nMobileye Drive\u2122\nFeatures\nVersatile full-stack self-driving system for integration into various types of vehicles, for both automakers and transportation operators\nDeployable in diverse operational domains around the world\nCustomizable to local driving culture, rules, and conventions\nExplore Mobileye\u2019s products and technology\nADAS\nBase Driver-Assist\nHands-on | Eyes-on\nCloud-Enhanced Driver-Assist\u2122\nHands-on | Eyes-on\nMobileye SuperVision\u2122\nHands-off | Eyes-on\nMobileye Chauffeur\u2122\nHands-off | Eyes-off\nMobileye Drive\u2122\nNo driver\nAV\nBase Driver-Assist\nAbout\nMobileye\u2019s Base Driver-Assist, powered by our purpose-built EyeQ system-on-chip, brings our Advanced Driver Assistance Systems (ADAS) to millions of vehicles on the road today. This solution provides cost-effective features for regulatory compliance and improved road safety.\nCUSTOMERS\nMore than 50 of the world\u2019s leading automakers, with over 800 models on the market around the world.\nBase Driver-Assist\nSensors\n1 front camera:\n120\u00b0 X 57 (+42,-15) FoV/up to 8MP\nBase Driver-Assist\nUnique Mobileye Tech\nMoblieye SoC:\nEyeQ\u2122 4M | EyeQ\u2122 5M | EyeQ\u2122 6L\nCamera-Based Technology\nMobileye\u2019s proprietary computer vision technologies\nBase Driver-Assist\nFeatures\nNCAP functions \u200b(Automatic Emergency Braking, Forward Collision Warning, Traffic Sign Recognition, \u200bLane Keep Assist)\u200b\u200b\nVision-only GSR ISA compliance\nAutomatic Cruise Control (ACC)\u200b\u200b\nLane centering on marked roads\u200b\u200b\nLane departure warning\u200b\u200b\nBase Driver-Assist\nVideo\nCloud-Enhanced Driver-Assist\u2122\nAbout\nMobileye Cloud-Enhanced Driver-Assist\u2122 leverages crowdsourced data from millions of Mobileye-equipped vehicles around the globe every day, providing centimeter-level localization through continuously updated information about the driving scene. It provides a safer, smoother, and more natural driving experience \u2013 marking a leap in ADAS performance but with no need for additional hardware.\nCUSTOMERS\nVWSkodaSEATFord\nCloud-Enhanced Driver-Assist\u2122\nSensors\n1 front camera:\n120\u00b0 X 57 (+42,-15) FoV/up to 8MP\nCloud-Enhanced Driver-Assist\u2122\nUnique Mobileye Tech\nMoblieye SoC:\nEyeQ\u2122 4M | EyeQ\u2122 5M | EyeQ\u2122 6L\nCamera-Based Technology\nMobileye\u2019s proprietary computer vision technologies\nRoad Experience Management\u2122 (REM)\nMobileye\u2019s crowdsourced AV mapping technology\nCloud-Enhanced Driver-Assist\u2122\nFeatures\nMobileye Base Driver Assist features plus:\u200b\u200b\nLane centering on unmarked roads\u200b\nTraffic light/sign relevancy \u200b& warning\nACC at high speeds, under changing road conditions\u200b\u200b\nOccluded traffic sign support\nAdvanced construction area and road/hazard warnings\nAutomatic speed adjustment according to traffic signs\nCloud-Enhanced Driver-Assist\u2122\nVideo\nMobileye SuperVision\u2122\nAbout\nMobileye SuperVision\u2122 is our most advanced driver-assist system on the market and the \u2018bridge\u2019 to consumer AVs. It is designed to handle standard driving functions across various road types, offering the \u201chands-off\u201d navigation capabilities of an autonomous vehicle, while still requiring the driver to pay full attention and keep eyes on the road.\nCUSTOMERS\nPorschePolestarFAWZeekrsmart\nMobileye SuperVision\u2122\nSensors\n11 cameras, including:\n7 long-range cameras 8MP\n4 parking cameras 3MP*Additional radars are optional\nMobileye SuperVision\u2122\nUnique Mobileye Tech\nMoblieye SoC:\nMobileye\u2019s ECU based on 2 x EyeQ\u2122 5/6H\nCamera-Based Technology\nMobileye\u2019s proprietary computer vision technologies\nRoad Experience Management\u2122 (REM)\nMobileye\u2019s crowdsourced AV mapping technology\nResponsibility-Sensitive Safety\u2122 (RSS)\nAn open source, comprehensive, and verifiable mathematical approach to balance safety and efficient driving\nMobileye SuperVision\u2122\nFeatures\nMobileye Cloud-Enhanced ADAS\u200b\u2122 features plus:\u200b\u200b\nHands-off navigation for various ODDs - included in-scope, highways, arterial, rural, and urban roads, at up to 130 kph\u200b\nAutomatic lane change\u200b\u200b\nAcceleration/deceleration in response to other drivers cutting in\u200b\u200b\nFront & rear collision avoidance\u200b\u200b\nAutomatic minimum risk maneuver\u200b\u200b\nDMS, parking & other applications on a single EyeQ\u2122 chip\u200b\u200b\nOTA upgrades\nDriving Experience Platform\u2122 (DXP\u2122)\nMobileye SuperVision\u2122\nVideo\nMobileye Chauffeur\u2122\nAbout\nMobileye Chauffeur\u2122 is our safe and geographically scalable hands-off/eyes-off solution for consumer vehicles, combining computer vision technology with surround imaging radars and front lidar.\nCUSTOMERS\nPolestarFAW\nMobileye Chauffeur\u2122\nSensors\n11 cameras, including:\n7 long-range cameras up to 8MP\n4 parking cameras 3MPand\n5 surround imaging radars\n1 front lidar\nMobileye Chauffeur\u2122\nUnique Mobileye Tech\nMoblieye SoC:\nMobileye's ECU Based on 3 or 4 x EyeQ\u2122 6H\nCamera-Based Technology\nMobileye\u2019s proprietary computer vision technologies\nRoad Experience Management\u2122 (REM)\nMobileye\u2019s crowdsourced AV mapping technology\nResponsibility-Sensitive Safety\u2122 (RSS)\nAn open source, comprehensive, and verifiable mathematical approach to balance safety and efficient driving\nTrue Redundancy\u2122\nMobileye\u2019s sensor fusion architecture based on two distinct independent systems: a camera system, and a radar-lidar system\nMobileye Chauffeur\u2122\nFeatures\nMobileye SuperVision\u2122 functions plus:\u200b\u200b\nEyes-off NOP platform for point-to-point navigation in various ODDs, including highways (up to 130 kph), arterials, rural, and urban roads\nAutomatic safe stop on road shoulder\nMobileye Chauffeur\u2122\nVideo\nMobileye Drive\u2122\nAbout\nMobileye Drive\u2122 is our end-to-end self-driving system that enables automakers and transportation operators to offer a no-driver solution for robotaxis, ride-pooling, public transport, and goods delivery.\nCUSTOMERS\nAutomakersVWMANSchaefflerBenteler/ HOLONBeepTransportation operatorsHolo/ RuterSIXTDeutsche- Bahn\nMobileye Drive\u2122\nSensors\n13 cameras, including:\n4 side cameras 8MP4 parking cameras 2MP3 front-facing cameras up to 8MP1 traffic light camera 8MP1 rear camera 8MPand5 surround imaging radars3 surround lidars\nMobileye Drive\u2122\nUnique Mobileye Tech\nMoblieye SoC:\nMobileye's ECU Based on 4 x EyeQ\u2122 6H\nCamera-Based Technology\nMobileye\u2019s proprietary computer vision technologies\nRoad Experience Management\u2122 (REM)\nMobileye\u2019s crowdsourced AV mapping technology\nResponsibility-Sensitive Safety\u2122 (RSS)\nAn open source, comprehensive, and verifiable mathematical approach to balance safety and efficient driving\nTrue Redundancy\u2122\nMobileye\u2019s sensor fusion architecture based on two distinct independent systems: a camera system, and a radar-lidar system\nMobileye Drive\u2122\nFeatures\nVersatile full-stack self-driving system for integration into various types of vehicles, for both automakers and transportation operators\nDeployable in diverse operational domains around the world\nCustomizable to local driving culture, rules, and conventions\nMobileye Drive\u2122\nVideo\nSign up for our newsletter\nIncorrect email address\nTune in to Mobileye CES posts on social media\nProf. Amnon Shashua\u2019s photo by Yanai Yechiel photographer\n\u00a9 2023 Mobileye\n\u00a9 2023 Mobileye"
      },
      {
        "title": "CES 2024: Mobileye (MBLY) Unveils DXP Operating System for Self-Driving ...",
        "url": "https://www.bloomberg.com/news/articles/2024-01-09/mobileye-unveils-dxp-operating-system-for-self-driving-cars",
        "content": "January 9, 2024 at 9:26 AM PST. Listen. 0:53. Mobileye Global Inc. introduced an operating system called DXP that's designed to help automakers develop customized self-driving systems and save ...",
        "score": 0.89091,
        "raw_content": "Bloomberg\nConnecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world\nFor Customers\nSupport\nAmericas+1 212 318 2000\nEMEA+44 20 7330 7500\nAsia Pacific+65 6212 1000\nCompany\nCommunications\nFollow\nProducts\nIndustry Products\nMedia\nMedia Services\nCompany\nCommunications\nFollow\nProducts\nIndustry Products\nMedia\nMedia Services\nBloomberg\nConnecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world\nFor Customers\nSupport\nAmericas+1 212 318 2000\nEMEA+44 20 7330 7500\nAsia Pacific+65 6212 1000\nBloomberg TV+\nBloomberg Markets China Open\nBloomberg Markets: China Open is the definitive guide to the markets in Hong Kong and on the mainland. David Ingles and Yvonne Man bring you the latest news and analysis to get you ready for the trading day.\nBloomberg Radio\nBloomberg Daybreak Asia\nLive market coverage co-anchored from Hong Kong and New York. Overnight on Wall Street is daytime in Asia. Markets never sleep, and neither does Bloomberg. Track your investments 24 hours a day, around the clock from around the world.\nBloomberg Originals\nAI IRL\nFrom art to sentience, space to war, AI IRL will examine the difference machine learning will make to all our lives.\nAlso streaming on your TV:\nMarkets\nSingapore Home Rents Fall for First Time in Over Three Years\nMarkets\nOil Set for Best Week Since October on Rising Geopolitical Risks\nMarket Data\nEconomics\nUK Consumer Confidence Hits Two-Year High as Inflation Cools\nEconomics\nTokyo Inflation Cools Below 2% in Sharper-Than-Forecast Move\nThe Big Take DC\nEconomists May\u00a0Be Using Bad Data to Make Big Decisions\nTelecom\nSatellite-Linked Phones Possible This Year, T-Mobile Chief Says\nTechnology\nT-Mobile Earnings Miss Overshadows Strong Subscriber Boost\nFeatured\nTechnology\nIntel Plunges After Bleak Forecast Casts Doubt on Comeback Bid\nTechnology\nKhazanah in Talks to Lead Oyo Hotel\u2019s $400 Million Fund Raise\nTechnology\nPayPal Drops Most Since August After Upgrades Fall Short\nAI\nGoogle to Team Up With Startup Hugging Face to Host AI Software\nTechnology\nNvidia, TSMC CEOs Meet as Global AI Chip Supply Remains Tight\nEconomics\nAI Needs So Much Power That Old Coal Plants Are Sticking Around\nPolitics\nSouth Korean Lawmaker Treated for Head Cuts After Being Attacked with Rock\nPolitics\nMali Junta Ends Peace Agreement With Separatist Rebels\nFeatured\nWealth\nMoney-Fund Assets Fall for Second Week on Portfolio Reallocation\nFinance\nBank of America Employees to Share $800 Million Stock Reward\nFeatured\nPursuits\nMan arrested outside Taylor Swift's NYC home held without bail for violating protective order\nPursuits\nRaheem Morris hired as head coach by Atlanta Falcons, who pass on Bill Belichick\nFeatured\nJessica Karl | Columnist\nPatriotic Millionaires Want Higher Taxes, Please\nAndy Mukherjee | Columnist\nIndia\u2019s Lending\u00a0Boom Can Be Safer, If Not Slower\nHal Brands | Columnist\nGood Economic News Keeps\u00a0Putting US Ahead in Cold War II\nThe Big Take\nHow a Lucky Break Fueled Eli Lilly\u2019s $600 Billion Weight-Loss Empire\nEconomics\nAI Needs So Much Power That Old Coal Plants Are Sticking Around\nBusiness\nWhy Did Car Insurance Get So Expensive?\nEquality\nVince McMahon Accused of Sex Trafficking by Ex-WWE Employee\nEquality\nTusk Proposes Lifting Europe\u2019s Tightest Abortion Restrictions\nFeatured\nNew Energy\nSolar Factories Slow Output as Overcapacity Weighs on Profits\nGreen\nAustralia Braces For Further Wild Weather in Wake of Ex-Tropical Cyclone Kirrily\nFeatured\nCityLab\nAmsterdam Mayor Calls for Regulation of Cocaine as Crime Thrives\nCityLab\nAtlanta\u2019s Squatter Problem Is Vexing Wall Street Landlords\nTransportation\nNew York City Set to Launch\u00a0\u2018Department of Sustainable Delivery\u2019\nCrypto\nBitcoin Spot ETFs Are Starting to See Slowing Investor Interest\nCrypto\nCrypto Fans Lured by 20% Stablecoin Yields Even After 2022 Bust\nCentral Banks\nBritain Starts Design Work on a Digital Version of the Pound\nSelf-Driving Cars:\nMobileye Unveils Customizable\u00a0Operating System for Self-Driving Cars\nMobileye Global Inc. introduced an operating system called DXP that\u2019s designed to help automakers develop customized self-driving systems and save them the time of building costly programs on their own.\n\u201cDXP is a driving experience platform,\u201d Mobileye Chief Executive Officer Amnon Shashua said in a Bloomberg Television interview. The software includes the universal components that all automakers need and lets them add their own customization, he said, speaking from the CES conference in Las Vegas."
      }
    ],
    "response_time": 5.53
  }
}